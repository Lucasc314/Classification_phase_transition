{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final model, we will be having a look at the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import datasets\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "X_train_reduced = np.load(\"X_train_reduced.npy\")\n",
    "X_test_reduced = np.load(\"X_test_reduced.npy\")\n",
    "\n",
    "#Flatten layers\n",
    "shape1 = X_train.shape\n",
    "shape2 = X_test.shape\n",
    "\n",
    "X_train = X_train.reshape(shape1[0], shape1[1]*shape1[2])\n",
    "\n",
    "X_test = X_test.reshape(shape2[0], shape2[1]*shape2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to produce different random forrests with different number estiamtors\n",
    "def rf(n, X, y, X_test, y_test, red, maxdepth = None):\n",
    "    regressor = RandomForestClassifier(n_estimators=n, max_depth = maxdepth)\n",
    "    start_time = time.time()\n",
    "    regressor.fit(X, y)\n",
    "    duration = (time.time() - start_time)\n",
    "    print(\"For the \" + red + \" dataset:\")\n",
    "    print(\"the random forest took \" + str(duration) + \" seconds for \" + str(n) + \" estimators.\")\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    sc = regressor.score(X_test, y_test)\n",
    "    print(\"the score of random forest with \" + str(n) + \" esitmators is: \" + str(sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first of all try a random forrest with 100 estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the non reduced dataset:\n",
      "the random forest took 9.950697898864746 seconds for 100 estimators.\n",
      "the score of random forest with 100 esitmators is: 0.9662839248434238\n"
     ]
    }
   ],
   "source": [
    "rf(100, X_train, y_train, X_test, y_test, \"non reduced\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at the dimensionally reduced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the reduced dataset:\n",
      "the random forest took 6.881294250488281 seconds for 100 estimators.\n",
      "the score of random forest with 100 esitmators is: 0.9875782881002088\n"
     ]
    }
   ],
   "source": [
    "rf(100, X_train_reduced, y_train, X_test_reduced, y_test, \"reduced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score and computational costs of the reduced dataset is much better. The total preformance is the best here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the non reduced dataset:\n",
      "the random forest took 5.112277030944824 seconds for 50 estimators.\n",
      "the score of random forest with 50 esitmators is: 0.9663883089770355\n"
     ]
    }
   ],
   "source": [
    "rf(50, X_train, y_train, X_test, y_test, \"non reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the reduced dataset:\n",
      "the random forest took 3.6399359703063965 seconds for 50 estimators.\n",
      "the score of random forest with 50 esitmators is: 0.9867432150313152\n"
     ]
    }
   ],
   "source": [
    "rf(50, X_train_reduced, y_train, X_test_reduced, y_test, \"reduced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of estimators does not improve the score, but only increases time. Let's have a look if a reduciton in estimators significantly decreases the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the non reduced dataset:\n",
      "the random forest took 2.1383020877838135 seconds for 20 estimators.\n",
      "the score of random forest with 20 esitmators is: 0.966910229645094\n"
     ]
    }
   ],
   "source": [
    "rf(20, X_train, y_train, X_test, y_test, \"non reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the reduced dataset:\n",
      "the random forest took 1.5109620094299316 seconds for 20 estimators.\n",
      "the score of random forest with 20 esitmators is: 0.9856993736951983\n"
     ]
    }
   ],
   "source": [
    "rf(20, X_train_reduced, y_train, X_test_reduced, y_test, \"reduced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing the number of estimators by a factor of 2 also decreases the time by a factor of 2. However, in both cases, reduced and non reduced, the score decreased significantly. This leads us to believe that the default number of estimators of 20 is best. Again the reduced dataset has a significantly better score that that of the non reduced. One more way to decrease computational cost is to introduce a maximal depth. Let's have a look how a maxdepth influences time and score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the non reduced dataset:\n",
      "the random forest took 2.1836540699005127 seconds for 20 estimators.\n",
      "the score of random forest with 20 esitmators is: 0.9665970772442589\n"
     ]
    }
   ],
   "source": [
    "rf(20, X_train, y_train, X_test, y_test, \"non reduced\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the reduced dataset:\n",
      "the random forest took 0.9571869373321533 seconds for 20 estimators.\n",
      "the score of random forest with 20 esitmators is: 0.9824634655532359\n"
     ]
    }
   ],
   "source": [
    "rf(20, X_train_reduced, y_train, X_test_reduced, y_test, \"reduced\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max depth of 5 on the both data sets improves time a bit, but decreases the score significantly. One final way to improve performance is to preprocess the data and make them Standard Scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepairing the data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_test_reduced = sc.fit_transform(X_test_reduced)\n",
    "X_train_reduced = sc.transform(X_train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the non reduced dataset:\n",
      "the random forest took 2.1309092044830322 seconds for 20 estimators.\n",
      "the score of random forest with 20 esitmators is: 0.9670146137787057\n"
     ]
    }
   ],
   "source": [
    "rf(20, X_train, y_train, X_test, y_test, \"non reduced\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the reduced dataset:\n",
      "the random forest took 1.298051118850708 seconds for 20 estimators.\n",
      "the score of random forest with 20 esitmators is: 0.9852818371607516\n"
     ]
    }
   ],
   "source": [
    "rf(20, X_train_reduced, y_train, X_test_reduced, y_test, \"reduced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again did not improve the preformance at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To conclude:\n",
    "The best preforming regressor is the default random forest on the reduced dataset with 20 estimators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}